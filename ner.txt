pip install torchtext==0.15.2 torch==2.0.1
pip install portalocker>=2.0.0
import torch.nn as nn
from torch import optim
from torch.nn.utils.rnn import pad_sequence
from torchtext.data.utils import get_tokenizer
from torch.utils.data import DataLoader
from torchtext.vocab import build_vocab_from_iterator
import pandas as pd
import numpy as np
import torch
if torch.cuda.is_available():
    device=torch.device(type='cuda', index=0)
else:
    device=torch.device(type='cpu', index=0)
tokenizer=get_tokenizer('basic_english', language='en')
data=pd.read_csv('/kaggle/input/named-entity-recognition-ner-corpus/ner.csv', header='infer').values
sentences=[]
targets=[]
for i in data:
    sentences.append(i[1])
    target=eval(i[-1])
    targets.append(target)
for i in targets:
    for j in range(len(i)):
        if i[j]=="O":
            i[j]=0
        else:
            i[j]=1
inputs=[]
for i in range(len(sentences)):
    inputs.append((sentences[i],targets[i]))
print(len(inputs))
special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']
def yield_tokens(data):
    for d in data:
        yield(tokenizer(d))
vocab=build_vocab_from_iterator(yield_tokens(sentences),
                               min_freq=1,
                               specials=special_symbols,
                               special_first=True)
vocab.set_default_index(0)
print(vocab.__len__())
def collate_fn(batch):
    src_batch, tgt_batch = [], []
    for src, tgt in batch:
        src = src.strip("\n")
        src_ids = vocab(tokenizer(src))

        if len(src_ids) > len(tgt):
            tgt.extend([0] * (len(src_ids) - len(tgt)))
        elif len(src_ids) < len(tgt):
            tgt = tgt[:len(src_ids)]

        src_batch.append(torch.tensor(src_ids, dtype=torch.long))
        tgt_batch.append(torch.tensor(tgt, dtype=torch.long))

    src_batch = pad_sequence(src_batch, padding_value=1, batch_first=True)
    tgt_batch = pad_sequence(tgt_batch, padding_value=1, batch_first=True)
    
    return src_batch, tgt_batch
dl=DataLoader(inputs, shuffle=True, batch_size=16, collate_fn=collate_fn)
class NER(nn.Module):
    def __init__(self, input_size, embed_size, hidden_size):
        super().__init__()
        self.e=nn.Embedding(input_size, embed_size)
        self.dropout=nn.Dropout(0.1)
        self.gru=nn.GRU(embed_size, hidden_size, batch_first=True)
        self.linear=nn.Linear(hidden_size, 2)
    def forward(self, x):
        x=self.e(x)
        x=self.dropout(x)
        output, hidden=self.gru(x)
        y=self.linear(output)
        return y
embed_size=128
hidden_size=128
ner=NER(vocab.__len__(), embed_size, hidden_size).to(device)
loss_fn=nn.CrossEntropyLoss(ignore_index=1).to(device)
lr=0.001
opt=optim.Adam(params=ner.parameters(), lr=lr)
def train_one_epoch():
    ner.train()
    losses = []
    
    for i, (s_ids, t_ids) in enumerate(dl):
        s_ids = s_ids.to(device)
        t_ids = t_ids.to(device)
        
        logits = ner(s_ids)
        logits_reshaped = logits.view(-1, logits.shape[-1])
        t_ids_reshaped = t_ids.view(-1)
        
        loss = loss_fn(logits_reshaped, t_ids_reshaped)
        losses.append(loss.item())  # Append the batch loss to the list
        
        if i % 1000 == 0:  # Print progress every 1000 batches
            print("Batch ", i + 1, round(np.mean(losses), 4))
        
        opt.zero_grad()
        loss.backward()
        opt.step()
    
    epoch_loss = round(np.mean(losses), 4)  # Compute the mean of all batch losses
    return epoch_loss
def eval_one_epoch():
    ner.eval()
    track_loss=0
    with torch.no_grad():
        for i, (s_ids, t_ids) in enumerate(dl):
            s_ids=s_ids.to(device)
            t_ids=t_ids.to(device)
            logits=ner(s_ids)
            logits_reshaped=logits.view(-1, logits.shape[-1])
            t_ids_reshaped=t_ids.view(-1)
            loss=loss_fn(logits_reshaped, t_ids_reshaped)
            track_loss+=loss.item()
            running_loss=round(track_loss/(i+(s_ids.shape[0]/16)),4)

    epoch_loss=running_loss
    return epoch_loss
num_epochs=1
for e in range(num_epochs):
    print("Epoch: ", e+1)
    epoch_loss=train_one_epoch()
    print("Training loss: ", epoch_loss)
    val_loss=eval_one_epoch()
    print("Validation loss: ", val_loss)
